> 偶然看到这本书的中文版，想到之前看英文版有些地方理解的不是很透彻，遂决定重读一遍，由于是挑选着章节进行阅读，所以本文会根据阅读进度不定期更新，小节编号对应的在原书中的章节。

## 8 降维

**维数灾难**

高维数据集有很大风险分布的非常稀疏：大多数训练实例可能彼此远离。当然，这也意味着一个新实例可能远离任何训练实例，这使得预测的可靠性远低于我们处理较低维度数据的预测，因为它们将基于更大的推测。训练集的维度越高，过拟合的风险就越大

**降维的两种主要方法**

- 投影

  大多数现实生活的问题中，训练实例并不是在所有维度上均匀分布的，许多特征几乎是常数，而其他特征则高度相关。结果，所有训练数据实际上位于高维空间的低维子空间内。

- 流行学习

  流形是指连接在一起的区域，流形学习基于一个假设，有意义的输入只分布在包含少量数据点的子集构成一组流行中，现实中有许多实例支持这种假设：图像、文本、声音的概率分布都是高度集中的，均匀的噪声与这类领域的结构化输入差异很大。

**几种典型的降维方法**

- PCA

- 核PCA

- LLE局部线性嵌入

  计算量太大了

**其他降维方法**

- MDS(Multidimensional Scalling)多维缩放

- ISO

- t-SNEt分布随机领域嵌入

- LDA线性判别分析

  LDA实际上一种分类算法，学习类之间最具区别性的轴，然后用这些轴来定义用于投影数据的超平面

## 5 支持向量机

非常强大并且有许多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。尤其适用于复杂但中小规模数据集的分类问题。

SVM对特征缩放敏感

- 硬间隔分类

  第一只对线性可分数据起作用，第二对异常点敏感

- 软间隔分类

  增加惩罚参数$c$,$c$值大时，对误分类的惩罚加大，模型倾向于会选择更小的边界间隔以减少误分类点，造成过拟合，反之变小，所以如果模型过拟合，那么不妨适当减小参数$c$

- 非线性支持向量机分类

  某些简单的数据集不是线性可分的，但如果转换特征编程线性可分的了

- SVM回归

- 常用核函数

  - Linear：$K(\textbf{a},\textbf{b})=\textbf{a}^T \cdot \textbf{b}$
  - Polynomial：$K(\textbf{a},\textbf{b})=(\gamma\textbf{a}^T \cdot \textbf{b}+r)^d$
  - Gaussian RBF：$K(\textbf{a},\textbf{b})=\exp(-\gamma||\textbf{a} - \textbf{b}||^2)$
  - Sigmoid：$K(\textbf{a},\textbf{b})=tanh(\gamma\textbf{a}^T \cdot \textbf{b}+r)$

  假设原始的数据时非线性的，我们通过一个映射 ϕ(⋅) 将其映射到一个高维空间中，数据变得线性可分

  核技巧的想法是，在学习与预测中定义核函数$K$,而不显示的定义映射函数$\phi$

  正定核函数


- 对偶问题


## 15 自编码器

训练目标

网络结构

训练数据集组成





## reference

- [书籍中文版](https://wizardforcel.gitbooks.io/hands-on-ml-with-sklearn-and-tf/content/docs/8.%E9%99%8D%E7%BB%B4.html)
- 



## 练习

### 1

## 2

### 3

#### 4

### 5

1. 支持向量机背后的基本思想是什么

   - 最大化决策边界的间隔
   - 针对非线性数据集使用核技巧

2. 什么是支持向量

   正好落在决策边界的向量

3. 当使用 SVM 时，为什么标准化输入很重要？

   不进行标准化，小的输入容易被忽略

4. 分类一个样本时，SVM 分类器能够输出一个置信值吗？概率呢？

   可以输出实例到决策边界的距离，但是无法输出概率，额外使用Logistic Regression

5. 在一个有数百万训练样本和数百特征的训练集上，你是否应该使用 SVM 原始形式或对偶形式来训练一个模型？

   只能用线性SVM的原始形式，SVM的原始形式的计算复杂度与训练集的实例数成比例增长，对偶形式的计算复杂度与$m^2$和$m^3$成比例增长，而对偶形式才能使用核技巧

6. 假设你用 RBF 核来训练一个 SVM 分类器，如果对训练集欠拟合：你应该增大或者减小$\gamma$？调整参数`C`呢？

   $\gamma$越大，决策边界越曲折，反之越平滑，c是对误分类点的惩罚，增大c加强对数据的拟合，因而二者都增大

7. 使用现有的 QP 解决方案，你应该怎么样设置 QP 参数（`H`，`f`，`A`，和`b`）去解决一个软间隔线性 SVM 分类器问题？

8. 在一个线性可分的数据集训练一个`LinearSVC`，并在同一个数据集上训练一个`SVC`和`SGDClassifier`，看它们是否产生了大致相同效果的模型。

9. 在 MNIST 数据集上训练一个 SVM 分类器。因为 SVM 分类器是二元的分类，你需要使用一对多（one-versus-all）来对 10 个数字进行分类。你可能需要使用小的验证集来调整超参数，以加快进程。最后你能达到多少准确度？

10. 在加利福尼亚住宅（California housing）数据集上训练一个 SVM 回归模型

### 6

### 7

### 8

1. 减少数据集维度的主要动机是什么？主要缺点是什么？
2. 什么是维度爆炸？
3. 一旦对某数据集降维，我们可能恢复它吗？如果可以，怎样做才能恢复？如果不可以，为什么？
4. PCA 可以用于降低一个高度非线性对数据集吗？
5. 假设你对一个 1000 维的数据集应用 PCA，同时设置方差解释率为 95%，你的最终数据集将会有多少维？
6. 在什么情况下你会使用普通的 PCA，增量 PCA，随机 PCA 和核 PCA？
7. 你该如何评价你的降维算法在你数据集上的表现？
8. 将两个不同的降维算法串联使用有意义吗？
9. 加载 MNIST 数据集（在第 3 章中介绍），并将其分成一个训练集和一个测试集（将前 60,000 个实例用于训练，其余 10,000 个用于测试）。在数据集上训练一个随机森林分类器，并记录了花费多长时间，然后在测试集上评估模型。接下来，使用 PCA 降低数据集的维度，设置方差解释率为 95%。在降维后的数据集上训练一个新的随机森林分类器，并查看需要多长时间。训练速度更快？接下来评估测试集上的分类器：它与以前的分类器比较起来如何？
10. 使用 t-SNE 将 MNIST 数据集缩减到二维，并使用 Matplotlib 绘制结果图。您可以使用 10 种不同颜色的散点图来表示每个图像的目标类别。或者，您可以在每个实例的位置写入彩色数字，甚至可以绘制数字图像本身的降维版本（如果绘制所有数字，则可视化可能会过于混乱，因此您应该绘制随机样本或只在周围没有其他实例被绘制的情况下绘制）。你将会得到一个分隔良好的的可视化数字集群。尝试使用其他降维算法，如 PCA，LLE 或 MDS，并比较可视化结果。

### 9

### 10

### 11

### 12

### 13

### 14

### 15

1. 自动编码器主要用于什么任务

   - 特征抽取
   - 无监督预训练
   - 降维
   - 生成模型
   - 异常检测

2. 训练一个分类器，如果你有大量无标签数据和几千少量的标签

   首先使用所有的数据训练深度自编码器，然后使用自动编码器的编码层进行分类训练，如果标签数据很少，可以冻结复用层

3. 如果一个编码器完全重建了输入，这是不是一个好的编码器，如何衡量一个编码器的性能

   不一定，如果编码器只是重建了数据却没有学到任何有效模式的情况下；衡量编码器性能可以通过计算重构误差(输入和输出的MSE)，另外，如果是进行分类任务的预训练，还要根据分类的准确率来判定

4. 什么是undercomplete编码器和overcomplete编码器，二者各自的风险是什么

   编码层分别小于和大于输入输出层

5. 如何关联训练一个自动编码器

6. 展示底层特征的技术是什么，高层特征又如何展示呢

7. 什么是生成模型，举出一种生成自编码器的类型

   生成模型随机生成和训练样本相似的实例

### 16

## 常见问题 

1. 如何权衡偏差和方差？

2. 什么是梯度下降？

3. 解释一下过拟合和欠拟合，如何解决这两种问题？

4. 如何处理维度灾难？

5. 什么是正则化项。为什么要使用正则化，说出一些常用的正则化方法？

6. 讲解一下PCA原理

7. 为什么在神经网络中Relu激活函数会比Sigmoid激活函数用的更多？

8. 什么是数据标准化，为什么要进行数据标准化？

我认为这个问题需要重视。数据标准化是预处理步骤，将数据标准化到一个特定的范围能够在反向传播中保证更好的收敛。一般来说，是将该值将去平均值后再除以标准差。如果不进行数据标准化，有些特征（值很大）将会对损失函数影响更大（就算这个特别大的特征只是改变了1%，但是他对损失函数的影响还是很大，并会使得其他值比较小的特征变得不重要了）。因此数据标准化可以使得每个特征的重要性更加均衡。

9. **解释什么是降维，在哪里会用到降维，它的好处是什么？**

降维是指通过保留一些比较重要的特征，去除一些冗余的特征，减少数据特征的维度。而特征的重要性取决于该特征能够表达多少数据集的信息，也取决于使用什么方法进行降维。而使用哪种降维方法则是通过反复的试验和每种方法在该数据集上的效果。一般情况会先使用线性的降维方法再使用非线性的降维方法，通过结果去判断哪种方法比较合适。而降维的好处是：

（1）节省存储空间；

（2）加速计算速度（比如在机器学习算法中），维度越少，计算量越少，并且能够使用那些不适合于高维度的算法；

（3）去除一些冗余的特征，比如降维后使得数据不会既保存平方米和平方英里的表示地形大小的特征；

（4）将数据维度降到2维或者3维使之能可视化，便于观察和挖掘信息。

（5）特征太多或者太复杂会使得模型过拟合。

**10. 如何处理缺失值数据？**

数据中可能会有缺失值，处理的方法有两种，一种是删除整行或者整列的数据，另一种则是使用其他值去填充这些缺失值。在Pandas库，有两种很有用的函数用于处理缺失值：isnull()和dropna()函数能帮助我们找到数据中的缺失值并且删除它们。如果你想用其他值去填充这些缺失值，则可以是用fillna()函数。

**11. 解释聚类算法**

请参考*（https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68–*详细讲解各种聚类算法*）*

**12. 你会如何进行探索性数据分析(EDA)？**

EDA的目的是去挖掘数据的一些重要信息。一般情况下会从粗到细的方式进行EDA探索。一开始我们可以去探索一些全局性的信息。观察一些不平衡的数据，计算一下各个类的方差和均值。看一下前几行数据的信息，包含什么特征等信息。使用Pandas中的df.info()去了解哪些特征是连续的，离散的，它们的类型(int、float、string)。接下来，删除一些不需要的列，这些列就是那些在分析和预测的过程中没有什么用的。

比如：某些列的值很多都是相同的，或者这些列有很多缺失值。当然你也可以去用一些中位数等去填充这些缺失值。然后我们可以去做一些可视化。对于一些类别特征或者值比较少的可以使用条形图。类标和样本数的条形图。找到一些最一般的特征。对一些特征和类别的关系进行可视化去获得一些基本的信息。然后还可以可视化两个特征或三个特征之间的关系，探索特征之间的联系。

你也可以使用PCA去了解哪些特征更加重要。组合特征去探索他们的关系，比如当A=0，B=0的类别是什么，A=1，B=0呢？比较特征的不同值，比如性别特征有男女两个取值，我们可以看下男和女两种取值的样本类标会不会不一样。

另外，除了条形图、散点图等基本的画图方式外，也可以使用PDF\CDF或者覆盖图等。观察一些统计数据比如数据分布、p值等。这些分析后，最后就可以开始建模了。

一开始可以使用一些比较简单的模型比如贝叶斯模型和逻辑斯谛回归模型。如果你发现你的数据是高度非线性的，你可以使用多项式回归、决策树或者SVM等。特征选择则可以基于这些特征在EDA过程中分析的重要性。如果你的数据量很大的话也可以使用神经网络。然后观察ROC曲线、查全率和查准率。

**13. 你是怎么考虑使用哪些模型的？**

其实这个是有很多套路的。我写了一篇关于如何选择合适的回归模型，链接在这*（https://towardsdatascience.com/selecting-the-best-machine-learning-algorithm-for-your-regression-problem-20c330bad4ef）*。

**14. 在图像处理中为什么要使用卷积神经网络而不是全连接网络？**

这个问题是我在面试一些视觉公司的时候遇到的。答案可以分为两个方面：首先，卷积过程是考虑到图像的局部特征，能够更加准确的抽取空间特征。如果使用全连接的话，我们可能会考虑到很多不相关的信息。其次，CNN有平移不变性，因为权值共享，图像平移了，卷积核还是可以识别出来，但是全连接则做不到。

**15. 是什么使得CNN具有平移不变性？**

正如上面解释，每个卷积核都是一个特征探测器。所以就像我们在侦查一样东西的时候，不管物体在图像的哪个位置都能识别该物体。因为在卷积过程，我们使用卷积核在整张图片上进行滑动卷积，所以CNN具有平移不变性。

**16. 为什么实现分类的CNN中需要进行Max-pooling？**

Max-pooling可以将特征维度变小，使得减小计算时间，同时，不会损失太多重要的信息，因为我们是保存最大值，这个最大值可以理解为该窗口下的最重要信息。同时，Max-pooling也对CNN具有平移不变性提供了很多理论支撑，详细可以看吴恩达的benefits of MaxPooling*（https://www.coursera.org/learn/convolutional-neural-networks/lecture/hELHk/pooling-layers）*。

**17. 为什么应用于图像切割的CNN一般都具有Encoder-Decoder架构？**

Encoder CNN一般被认为是进行特征提取，而decoder部分则使用提取的特征信息并且通过decoder这些特征和将图像缩放到原始图像大小的方式去进行图像切割。

**18. 什么是batch normalization，原理是什么？**

Batch Normalization就是在训练过程，每一层输入加一个标准化处理。

深度神经网络之所以复杂有一个原因就是由于在训练的过程中上一层参数的更新使得每一层的输入一直在改变。所以有个办法就是去标准化每一层的输入。具体归一化的方式如下图，如果只将归一化的结果进行下一层的输入，这样可能会影响到本层学习的特征，因为可能该层学习到的特征分布可能并不是正态分布的，这样强制变成正态分布会有一定影响，所以还需要乘上γ和β，这两个参数是在训练过程学习的，这样可以保留学习到的特征。

![img](http://www.tensorflownews.com/wp-content/uploads/2018/07/1-6.png)

*来自网络*

神经网络其实就是一系列层组合成的，并且上一层的输出作为下层的输入，这意味着我们可以将神经网络的每一层都看成是以该层作为第一层的小型序列网络。这样我们在使用激活函数之前归一化该层的输出，然后将其作为下一层的输入，这样就可以解决输入一直改变的问题。

**19. 为什么卷积核一般都是3\*3而不是更大？**

这个问题在VGGNet模型中很好的解释了。主要有这2点原因：第一，相对于用较大的卷积核，使用多个较小的卷积核可以获得相同的感受野和能获得更多的特征信息，同时使用小的卷积核参数更少，计算量更小。第二：你可以使用更多的激活函数，有更多的非线性，使得在你的CNN模型中的判决函数有更有判决性。

**20. 你有一些跟机器学习相关的项目吗？**

对于这个问题，你可以从你做过的研究与他们公司的业务之间的联系上作答。 你所学到的技能是否有一些可能与他们公司的业务或你申请的职位有关？ 不需要是100％相吻合的，只要以某种方式相关就可以。这样有助于让他们认为你可以在这个职位上所产生的更大价值。

**21. 解释一下你现在研究生期间的研究？平时都在做什么工作？未来的方向是什么？**

这些问题的答案都跟20题的回答思路是一致的。

 



 


          
         

