# 机器学习的目标函数是如何生成的

> 通常在机器学习的分类和回归问题中，我们都会使用分别使用交叉熵损失和均方误差作为目标函数，可你是否考虑过这两类目标函数为何会有效，或者说它们是如何对优化参数产生作用的。

### 概率论视角

> 概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果；而似然性则是用在已知某些观测所得到的结果时，对有关事物的性质的**参数**进行估计。

首先来看贝叶斯定理：
$$
p(\vec w|D) = \cfrac{p(D|\vec w)p(\vec w)}{p(D)}\tag{1}
$$
通常情况下，会按照某个分布$p(\vec w)$对参数$\vec w$进行初始化,$p(\vec w)$称为$\vec w$的先验分布，而我们的目标是要得到$\vec w$的后验概率分布$p(\vec w|D)$，即观测到数据D后，参数$\vec w$在观测数据集D上的一个后验概率分布。



再来Baye定理的右边，$p(D|\vec w)$是似然函数，是关于参数$\vec w$的函数，表达已知有事件D发生，运用似然函数$L(\vec w|D)$，估计参数$\vec w$的可能性	，似然函数不满足归一性，似然性不等同于概率,但它是$\vec w = \vec w_i$时的条件概率，$p(D|\vec w)$表达的意义是，在观测到D时，$\vec w = \vec w_i$的似然性（是似然性而不是概率，似然函数不满足归一性，重要性质重复说多少遍都不为过）



如果此时$\vec w = \vec w_j(j\ne i)​$,那么似然函数的值也会改变，**似然函数的重要性不是它的具体取值，而是当参数变化时函数到底变小还是变大**。



自然的，引出极大似然估计：似然函数取得最大时表示相应的参数能够使得统计模型最为合理。



> PS：我觉得似然函数的值不是概率的另一个解释可以是，能不能称作概率取决于你当前所处的阶段，如果你是在基于观测数据D进行$\vec w$参数的估计，那么此时似然函数表达的是似然性而不是条件概率，而如果你是基于$\vec w = \vec w_i$的前提，去预测接下来将会出现的数据，那么此时似然函数那个表达式（这个时候也就不应当称作似然函数了，虽然还是同一个表达式）所计算的就是条件概率。这其实就回到了概率和似然性的定义了。



在每一轮的训练过程中，我们都可以假设在当前时刻下$\vec w = \vec w_i$是已知的，已知参数$\vec w$的情况下，D发生的概率为$p(D|\vec w) = \cfrac{p(\vec w,D)}{p(\vec w)}$，当然我们不这么计算似然函数，而是以此计算$p(\vec w,D)$.



> 根据Beya定理的加法原则，上式的分母$p(D)$可以表达为：
> $$
> p(D)=\int p(D|\vec w)p(\vec w)d\vec w \tag{2}
> $$
>

有了上面的基础，下面我们讨论一个本质的问题：机器学习问题最终都会转化为解目标函数的优化问题，那么我们有没有想过这个目标函数是如何生成的。

> MLE（Maximam Likelihood Estimation,极大似然估计）和MAP（Maximam a Posterior最大后验概率估计）是生成目标函数的很基本思想，我们需要对其有着深刻理解。

- [ ] 我们又有没有考虑过这样一些问题，为什么将均方差（MSE）和交叉熵损失分别作为回归和分类任务的目标函数？为什么增加一个正则项是有意义的？

      - [x] 分类任务的交叉损失熵是如何与MLE相关联的？会议下面这样几个概念，后面涉及会介绍。

            - [x] 交叉熵
            - [x] softmax
            - [x] KL divergence

      - [x] 回归问题的均方误差与最大后验概率分布的关系

      - [x] 正则化项的意义是什么

            MLE中添加的L2正则化项等价于MAP中使用高斯分布作为先验概率分布。

#### MLE和MAP

**针锋相对**：

不管是贝叶斯学派还是频率学派，似然函数都发挥着核心作用，但是二者的理解方式还是有着很大的不同。两者最本质的区别是看待世界的视角不同，频率学派认为与事物有关的参数是客观存在的，不会改变，虽然未知，但是是固定值；贝叶斯学派认为参数是随机的，首先有一个先验概率分布，然后需要根据事实（观测数据）去不断调整参数的概率分布，以找到能够对现实进行最优描述的概率分布。

MLE在小样本上偏差大，MAP先验不同给出的结果不同。

#### 理解目标函数

首先我们来看经常作为分类和回归任务目标函数的交叉熵损失和均方误差：
$$
CE = -\sum_x p(x)logq(x) \tag{3}
$$
其中，$p(x)$是正确的标注，$q(x)$是网络输出。
$$
MSE = \cfrac1N\sum_{i=1}^N (\hat y_i - y_i)^2 \tag{4}
$$
其中y是正确的标注，$\hat y$是网络输出。



##### 分类

二分类激活函数使用sigmoid(将输入映射到[0, 1]区间)，多分类激活函数使用softmax(对输出值归一化为概率值)

sigmoid:
$$
Sigmoid(x)={\frac {1}{1+e^{-wx}}}={\frac {e^{wx}}{e^{wx}+1}}\tag{5}
$$
![](/media/files/images/sigmoid.png)
softmax:
$$
\sigma : \mathbb{R}^{K}\to\{z\in\mathbb{R}^{K}|z_{i}>0,\sum_{i=1}^{K}z_{i}=1\}\tag{6}
$$

$$
\sigma (\mathbf {z} )_{j}={\frac {e^{z_{j}}}{\sum _{k=1}^{K}e^{z_{k}}}}  for j = 1, …, K.\tag{7}
$$

$$
{\displaystyle P(y=j\mid \mathbf {x} )={\frac {e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{j}}}{\sum _{k=1}^{K}e^{\mathbf {x} ^{\mathsf {T}}\mathbf {w} _{k}}}}}\tag{8}
$$

其实分类的损失函数和你分类正确与否没有直接关系，而是基于观察数据，在极大似然函数里加入标签对应类别的概率，不管是二分类还是多分类，最终正确的分类标签只有一个，在进行似然估计时也仅考虑观测数据。

对于二分类问题：

- [x] 对于输入x,类标签是t，我们的目标是找到$\vec w,y=sigmoid(\vec w \vec x)$使得$p(t|x)$最大:$p(t|x) = (y)^t(1-y)^{1-t}$,因为sigmoid函数默认输出的就是t=1时的概率，在进行极大似然估计时，会选择和观测数据的类别所对应的概率加入到似然函数中，对于所有的观测样本，似然函数可以表达为:
  $$
  L_{ce} = \prod_{i=1}^Np(t_i|x_i)=\prod_{i=1}^N(y_i)^{t_i}(1-y_i)^{1-t_i}\tag{9}
  $$




对于多分类问题，其中假定C是分类类别，$\vec t$为ont-hot向量，每个类别的输出概率计为$y_i=P(t_i|x)$：
$$
p(\vec t|x) =\prod_{i=1}^C (y_i)^{t_i}\tag{10}
$$

$$
Like = \prod_{i=1}^Np(\vec t_i|x)\tag{11}
$$

转化为最小化负对数似然函数：
$$
-logp(t|x) =-\sum_{i=1}^C{t_i}log (y_i)\tag{12}
$$

$$
-logLike = -\sum_{i=1}^Nln\ p(t|x)\tag{13}
$$



**交叉熵**

相对熵定义为，衡量使用$q(x)$近似$p(x)$时的信息损失：
$$
\begin{split}D_{KL}(p||q) & = \sum_{x \in X} p(x) log \frac{p(x)}{q(x)} \\& =\sum_{x \in X}p(x)log \ p(x) - \sum_{x \in X}p(x)log \ q(x) \\& =-H(p) - \sum_{x \in X}p(x)log\ q(x)\end{split}\tag{14}
$$
交叉熵定义为：
$$
CE(p, q) = -\sum_{x \in X}p(x)log\ q(x) = H(p) + D_{KL}(p||q)\tag{15}
$$


根据交叉熵的定义，我们可以得到二分类和多分类问题的交叉熵损失函数分别为：
$$
l_{bi}=-\sum_{i=1}^N (t_ilog\ y_i+(1-t_i)log\ (1-y_i))\tag{16}
$$

$$
l_{multi}=-\sum_{i=1}^N\sum_{j=1}^Ct_{ij}log (y_{ij})\tag{17}
$$

综上，分类问题的交叉熵损失函数与极小化负对数似然函数形式是一样的，因而这也就解释了交叉熵为什么在分类问题中是有效的。

<!--即便我们知道了这二者形式一样，也可以根据MLE去理解交叉熵损失函数的意义，但是为什么依据交叉熵定义写出的损失函数会和MLE的形式一样呢，交叉熵本身的定义里和极大似然函数是否有什么关系呢？-->



##### 回归

在深入回归问题的目标函数之前，我们先来看这样一张图：

![/images_md/awesome.png](/media/files/images/awesome.png)

这张图是学习机器学习以来给予我最大启发的一张图，那么我们应该如何理解这张图所要传达给我们的信息呢？

> 首先我们基于这样一种假设，目标值t服从以$y(x,w)$为均值的高斯分布，因而有：
> $$
> p(t|x,w,\beta)=N(t|y(x,w),\beta^{-1})\tag{18}
> $$
>

假设我们使用训练集$\{x,t\}$,如果假设数据间是独立，那么似然函数可以表达为：
$$
p(t|x,w,β) = \prod_{n=1}^NN( t_n|y(x_n, w), β^{−1})\tag{19}
$$

$$
-lnp(t|x,w,β)= -\cfrac{\beta}2\sum_{n-1}^N \{y(x_n,w)−t_n\}^2 + \cfrac N2 lnβ−\cfrac N 2 ln(2π).\tag{20}
$$

均方误差与假设t服从高斯分布并使用该分布对观测数据进行极大似然估计是一致的！换言之极大似然估计求的的参数值可以由使用反向传播的随机梯度下降算法优化均方误差得到。

因而对于模型输出的每一个预测值t，t取高斯分布最大概率时取值，即高斯分布的均值处，我们的目标就是基于观测数据来找到使似然函数取得最大值时的参数$\vec w*$，这也正是均方误差的意义所在。

<!--那么均方误差的提出是不是独立的呢？是先有的均方误差后来才碰巧发现了其所包含的假设和意义吗？-->

正则化项的意义则可以由最大后验概率估计来解释，L2正则化项可以由基于正态分布的先验假设来解释。
$$
p(w|x, t, α, β) \varpropto p(t|x, w, β)p(w|α).\tag{21}
$$
