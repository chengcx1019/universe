## Hsuan-Tien Linâ€”â€”Machine Learning Foundations

When can machines learnï¼ŸLecture1-4

Why can machines learnï¼ŸLecture5-8

How can machines learnï¼ŸLecture9-12

How can machines learn betterï¼ŸLecture13-16

#### Lecture1:The Learning Problem

A takes D and H to get g.

##### Course Introduction

##### What is Machine Learning

machine learning:improving some performance measure with experience from data.

é€šè¿‡ä»æ•°æ®è®¡ç®—å‡ºçš„ç»éªŒæ¥æå‡è¡¨ç°



key essenceï¼ˆæœ¬è´¨ï¼‰

1. å­˜åœ¨å¯ä»¥å­¦ä¹ çš„éšè—æ¨¡å¼
2. æ²¡æœ‰å¯æ˜“ç¼–ç¨‹çš„å®šä¹‰
3. æœ‰å¯ä¾›å­¦ä¹ æ¨¡å¼çš„æ•°æ®

##### Applications of Machine Learning

åº”ç”¨ï¼šè®¡ç®—ç»æµï¼Œå›¾ç‰‡è¯†åˆ«

##### Components of Machine Learning

use data  to compute hypothesis g that approximates target f

##### Machine Learning and other Fields

MLâ€”â€”Data Mining

ä¼ ç»Ÿçš„æ•°æ®æŒ–æ˜æ›´å…³å¿ƒå¤§æ•°æ®é‡çš„é«˜æ•ˆè®¡ç®—

MLâ€”â€”Artificial Intelligence

MLæ˜¯å®ç°AIçš„ä¸€ç§é€”å¾„

MLâ€”â€”Statistics

ä¼ ç»Ÿçš„ç»Ÿè®¡åˆ†æå…³æ³¨ç‚¹æ›´å¤šæ˜¯åœ¨ç»“æœçš„æ•°å­¦å‡è®¾è¯æ˜



#### Lecture2:Learning to Answer Yse/No

##### Perceptron Hypothesis Set

$h(x)=sign((\sum_{i=1}^dw_ix_i)-threshold)$

$=sign((\sum_{i=1}^dw_ix_i)+(-threshold)_{w0}*(+1)_{x0})$

$=sign(\sum_{i=0}^dw_ix_i))=sign(w^Tx)$

##### Perceptron Learning Algorithm(PLA)

åœ¨æ•°æ®é›†çº¿æ€§å¯åˆ†çš„åŸºç¡€ä¸Šï¼Œå¯»æ‰¾è¯¯åˆ†ç±»ï¼Œä¿®æ­£wç›´åˆ°æ²¡æœ‰è¯¯åˆ†ç±»ç‚¹

$sign(w^Tx_n)\neq y_n,w_{t+1}^T<â€”w_t+y_nx_n$

perceptronâ€”â€”linear

è¯¯åˆ†ç±»ç‚¹è·ç¦»æœ€å°

A fault confessed is half redressed.çŸ¥é”™èƒ½æ”¹ï¼Œå–„è«å¤§ç„‰

issues of PLAï¼ˆperceptron learning algorithmï¼‰ï¼š

1. ç®—æ³•ä½•æ—¶åœæ­¢ï¼ˆlinear separableçº¿æ€§å¯åˆ†ï¼‰
2. å‡†ç¡®ç‡åˆ°åº•æœ‰é«˜

$y_nw_{t+1}^Tx_n\geq y_nw_t^Tx_n$

##### Guarantee of PLA

è®¾ç»è¿‡Tæ¬¡è¿­ä»£$w_f$å°†è®­ç»ƒæ•°æ®é›†å®Œæˆæ­£ç¡®åˆ†å¼€

è®°$R^2=max_{1\leq i\leq n}||x_i||^2,\rho=min_iy_i\frac{W_f^T}{||w_f||}x_i$

$sign(w_t^Tx_{n(t)})\neq y_{n(t)},y_{n(t)}w^Tx_{n(t)}\leq 0$



$||w_{t+1}^T||^2=||w_t+y_{n(t)}x_{n(t)}||^2$

$=||w_t||^2+2y_{n(t)}w^Tx_{n(t)}+||y_{n(t)}x_{n(t)}||^2$

$\leq ||w_t||^2+0+||y_{n(t)}x_{n(t)}||^2$

$\leq ||w_t||^2+0+R^2$

$\leq ||w_{t-1}||^2+2R^2 \leq...$

$\leq (T+1)R^2$



$w_f^Tw_{t+1}=w_f^T(w_t+y_{n(t)}x_{n(t)})$

$\geq w_f^Tw_t +||w_f||\rho$

$\geq (T+1)||w_f||\rho$



$1\geq \frac{w_f^T}{||w_f||}\frac {w_T}{||w_T||}\geq \frac{T||w_f||\rho}{||w_f||\sqrt[2]TR}=\sqrt[2]T\frac{\rho}R$

$T\leq\frac{R^2}{\rho^2}$

##### Non-separable Data

å¦‚æœæ•°æ®é›†Dä¸æ˜¯çº¿æ€§å¯åˆ†ï¼Œæœ‰å™ªå£°æ•°æ®ï¼ˆnoisy dataï¼‰æ—¶è¯¥å¦‚ä½•å­¦ä¹ 

$w_g<â€”argmin_w\sum_{n=1}^N[[y_n\neq sign(w^Tx_n)]]$(çŠ¯é”™è¯¯æœ€å°çš„)

è¿™æ˜¯NP-hardé—®é¢˜

é€€è€Œæ±‚å…¶æ¬¡ï¼Œä½¿ç”¨pocket algorithmï¼Œæ¯æ¬¡ä¿ç•™æœ€å¥½çš„ï¼Œè¿­ä»£åˆ°ä¸€å®šæ¬¡æ•°å°±è®¤ä¸ºè¾¾åˆ°æœ€ä¼˜äº†ã€‚



#### Lecture3:Types of Learning

##### Learning wth Different Output Space Y

å¤šåˆ†ç±»

å›å½’

##### Learning wth Different Data Label $y_n$

###### Unsipervised Learning Problems

unsupervised multiclass classificationâ€”â€”clustering

æ²¡æœ‰æ ‡è®°ï¼Œèšç±»

Density estimation

Outline detection

###### Semi-supervised

åªæœ‰ä¸€éƒ¨åˆ†æ ‡è®°ï¼Œæ ‡è®°çš„ä»£ä»·å¤ªå¤§

###### Reinforcement Learning

##### Learning wth Different Protocol $f â‡’(x_n,y_n)$

batch:å¡«é¸­å¼ï¼Œæ‰€æœ‰æ•°æ®

onlineï¼šhypothesis improves through receiving data instances   sequentially.

active

random

##### Learning wth Different Input Space X

concrete featuresï¼šsophisticatedï¼ˆand relatedï¼‰ physical meaning

raw featuresï¼šsimple physical meaning

abstract featureï¼šno pyhsical meaningï¼Œ



#### Lecture4:Feasibility of Learning

##### Learning is Impossible

##### Probability to the Rescue

ğ¶æ˜¯æŠ½æ ·æŸä¸€ç±»çš„æ¦‚ç‡ï¼Œğµæ˜¯åŸå§‹æ•°æ®ä¸­è¯¥ç±»çš„æ¦‚ç‡ï¼Œä¸€èˆ¬è€Œè¨€ï¼Œğ¶ä¸ğµæœ‰ä»€ä¹ˆå…³ç³»ï¼Œå¯å¦ç”¨ğ¶æ¥æè¿°ğµï¼Ÿ

å½“Nçš„å€¼è¶³å¤Ÿå¤§ï¼Œåˆ™å¯ä»¥è¯´vä¸uæ˜¯æ¥è¿‘çš„

$P[|ğ¶-ğµ|>ğŸ„]\leq 2e^{-2NğŸ„^2}$

ä¸Šå¼ç§°ä¸ºHoeffdingâ€™s Inequalityï¼ˆéœå¤«ä¸ä¸ç­‰å¼ï¼‰

è¡¨è¾¾å¼$'ğ¶=ğµ'$çš„æ„ä¹‰æ˜¯å·®ä¸å¤šå¤§æ¦‚æ˜¯æ­£ç¡®çš„ï¼ˆprobably approximately correct,PACï¼‰

##### Connection to Learning

å‰æï¼šNè¶³å¤Ÿå¤§å¹¶ä¸”æ˜¯i.i.d ç‹¬ç«‹å–æ ·

Unknown $E_{out}(h)=\epsilon_{x~p}[[]h(x)\neq f(x)]$ 

by known $E_{in}(h)=\frac1N\sum_{n=1}^N[[h(x_n)\neq y_n]]$

$E_{in}$æ ·æœ¬é”™è¯¯ç‡ï¼Œ$E_{out}$æ ·æœ¬ä¹‹å¤–æ•°æ®çš„é”™è¯¯ç‡

$P[|E_{in}-E_{out}|>ğŸ„]\leq 2e^{-2NğŸ„^2}$

##### Connection to Real Learning

###### bad sample and bad data

$P_D[BAD D]= \sum_{all possible D}P(D)âˆ™[[BAD D]]$



$x_iç›¸äº’ç‹¬ç«‹,ä¸”x_i\in [a_i,b_i],S_n=x_1+x_2+Â·Â·Â·+x_n$$ åˆ™æœ‰:$$P(S_n-E[S_n]\geq t)\leq exp(-\frac {2t^2}{\sum_{1}^n{(b_i-a_i)^2}})$

###### Bound of Bad Data

$$P_D[BAD D]= P_D[BADD for h_1 or BADD for h_2â€¦orBADD for h_M]$$

$$\leq P_D[BADD for h_1]+P_D[BADD for h_2]+â€¦+P_D[BADD for h_M]$$

$$\leq2e^{-2N\epsilon^2}+2e^{-2N\epsilon^2}+â€¦+2e^{-2N\epsilon^2} $$

$$=2Me^{-2N\epsilon^2}$$



#### Lecture5:Training versus Testing

åˆ°åº•ä¸ºä»€ä¹ˆæœºå™¨å¯ä»¥å­¦åˆ°ä¸œè¥¿

##### Recap and preview

å­¦ä¹ çš„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜

1. $E_{out}(g)$æ˜¯å¦è¶³å¤Ÿæ¥è¿‘$E_{in}(g)$
2. èƒ½å¦ä½¿$E_{in}(g)$è¶³å¤Ÿå°

$M$å³$|H|$åœ¨ä»¥ä¸Šä¸¤ä¸ªé—®é¢˜ä¸­èµ·ç€ä»€ä¹ˆä½œç”¨

Må°å¾—æ—¶å€™ï¼šæ¥è¿‘ä½†é€‰æ‹©æœ‰é™

Må¤§çš„æ—¶å€™ï¼šå¾ˆéš¾ä¿è¯äºŒè€…æ¥è¿‘ï¼Œä½†æœ‰æ›´å¤šé€‰æ‹©



$P[|E_{in}-E_{out}|>ğŸ„]\leq 2Me^{-2NğŸ„^2}$

è®°$2Me^{-2NğŸ„^2}=\delta$,å¦‚æœä½¿$\delta=0.05$,å³5%çš„é”™è¯¯å®¹å¿æƒ…å†µä¸‹ï¼Œ$\epsilon=0.1.M=100$çš„æƒ…å†µä¸‹ï¼Œéœ€è¦çš„æ•°æ®å¤§å°åº”è¯¥å¤šå°‘ï¼Ÿ$N=\frac1{2\epsilon^2}\ln{\frac{2M}\delta}$

##### Effective Number of Lines

åœ¨hypothesis set $H={all\space  lines\space in\space R^2}$ä¸­,å¯ä»¥æ ¹æ®æ•°æ®é›†å¯¹çº¿è¿›è¡Œåˆ†ç±»ï¼Œæ‰¾åˆ°é‚£äº›çœŸæ­£æœ‰æ•ˆçš„çº¿ã€‚

æ‰¾å‡ºç›¸ä¼¼å‡è®¾ï¼Œåˆ†ç±»æƒ…å†µæ˜¯ä¸€è‡´çš„ï¼Œå¦‚1ä¸ªç‚¹ï¼Œ2ç§åˆ†ç±»ï¼Œ2ä¸ªç‚¹ï¼Œæœ€å¤š4ç§åˆ†ç±»æƒ…å†µï¼Œ3ä¸ªç‚¹ï¼Œæœ€å¤š8ç§åˆ†ç±»æƒ…å½¢ï¼ˆç‚¹å…±çº¿	ï¼‰ï¼Œ4ä¸ªç‚¹ï¼ˆä»…åˆ—ä¸¾8ç§æƒ…å†µ1111 1110  1101 1100 1000 1001 1010 1011ï¼‰

Nä¸ªè¾“å…¥æœ€å¤§åˆ†ç±»ç§æ•°å³æœ‰æ•ˆçš„çº¿çš„æ•°ç›®ï¼Œè®°ä¸ºï¼š$effective(N)$

$P[|E_{in}-E_{out}|>ğŸ„]\leq 2effective(N)e^{-2NğŸ„^2}$

##### Effective Number of Hypothese

###### Dichotomies:Mini-hypotheses

dichotomyäºŒåˆ†

|      | hypotheses H       | dichotomise $H(x_1,x_2,â€¦,x_n)$ |
| ---- | ------------------ | ------------------------------ |
| e.g. | all lines in $R^2$ | ç‚¹çš„å–å€¼ä¸åŒç»„åˆ                       |
| Size | possibly infinite  | Upper bounded by $2^N$         |

###### Growth Function

æˆé•¿å‡½æ•°(growth function)ï¼šå–ä¸åŒNæ—¶ç»„åˆçš„æœ€å¤§å€¼

$$m_H(N)=max_{x_1,x_2,â€¦,x_N\in X |H(x_1,x_2,â€¦,x_n)|}$$

å¦‚ä½•è®¡ç®—æˆé•¿å‡½æ•°ï¼Œä»¥ä¸‹æ˜¯å‡ ç§å…¸å‹æ¨¡å‹çš„æˆé•¿å‡½æ•°ï¼š

**positive rays**ï¼š$ h(x)=sign(xâˆ’a) $è¾“å…¥ç©ºé—´ä¸ºä¸€ç»´å‘é‡ï¼Œ å½“$x_iâ‰¥a$, è¾“å‡º +1, å½“$x_i<a$ æ—¶ï¼Œ è¾“å‡º -1. 

$m_H(N)=N+1$(Nä¸ªç‚¹å°†åŒºåŸŸåˆ’åˆ†ä¸ºï¼ˆN+1ï¼‰ä¸ªéƒ¨åˆ†)ï¼Œä½†å¦‚æœæœ‰ä¸¤ä¸ªç‚¹åŒæ—¶å‘ç”Ÿçš„è¯ï¼Œå°±æ²¡æœ‰è¿™ä¹ˆå¤šæƒ…å†µäº†ï¼Œä½†è¿™å¯¹æˆé•¿å‡½æ•°æ˜¯æ²¡æœ‰å½±å“çš„ï¼Œå› ä¸ºæˆé•¿å‡½æ•°æœ¬èº«æ˜¯å–åˆ†ç±»æœ€å¤šçš„æƒ…å†µã€‚

$m_H(2)=3\leq 2^2:break\space point\space at\space2$

**positive intervals**ï¼š$å½“ x_iâˆˆ[l,r),y_i=1;x_i\notin [l,r),y_i=âˆ’1$

$m_H(N)=(_2^{N+1})=\frac12N^2+\frac12N+1$

Nä¸ªç‚¹å°†åŒºåŸŸåˆ’åˆ†ä¸º N+1ä¸ªç©ºé—´ï¼Œ N+1ä¸ªç©ºé—´ä»»å–ä¸¤ä¸ªç©ºé—´è¿›è¡Œåˆ’åˆ†ï¼Œä½†å¦‚æœä¸¤è€…é‡åˆçš„è¯ï¼Œå¯¹åº”äºä¸€ç§åˆ’åˆ†(å‡ä¸º-1)ã€‚

$m_H(3)=7\leq 2^3:break\space point\space at\space3$

**convex set(å‡¸é›†)**ï¼šh(x)=+1iffxina convex region,  1 otherwise

$m_H(N)=2^N$

$m_H(N)=2^N\space always:no\space break\space point\space$

**2D perceptrons**:

$m_H(N)<2^N$

$m_H(4)=14\leq 2^4:break\space point\space at\space4$

**Positive and negative rays(1D perceptrons)**:ç­‰ä»·äº1ç»´çš„perceptron hypothsis

$n_H=2(N-1)+2$

Nä¸ªç‚¹ä¸­é—´çš„ï¼ˆN-1ï¼‰ä¸ªåŒºåŸŸå¯ä»¥åˆ†åˆ«æ­£å‘å’Œåå‘ï¼Œå…¨æ­£å…¨åå„ä¸€ï¼ˆä½†æ˜¯éƒ½å¯¹åº”äºè¾¹ç•ŒåŒºåŸŸçš„ä¸¤ç§æƒ…å†µæ­£æ­£-1ï¼Œè´Ÿè´Ÿ-1ï¼Œæ­£è´Ÿ+1ï¼Œè´Ÿæ­£+1ï¼‰

$m_H(3)=6\leq 2^3:break\space point\space at\space3$

##### Break Point

å¤šé¡¹å¼ï¼ˆpolynomialï¼‰è¿˜æ˜¯æŒ‡æ•°å‹ï¼ˆexponentialï¼‰

å¦‚æœ kä¸ªè¾“å…¥æ²¡æœ‰è¢«Hå®Œå…¨åˆ’åˆ†ï¼Œåˆ™ç§°kä¸ºHçš„break point



#### Lecture6: Theory of Generalization

##### Restriction of Break Point

> å‰é¢5ç§æ¨¡å‹çš„braek pointå‡åœ¨L5_groth functionä¸­ç»™å‡º

æˆé•¿å‡½æ•°ä¸­å¢åŠ break pointçš„é™åˆ¶

<u>maximum possible mH(N) when N = 3 and k = 2?</u>

å¦‚æœæœ€å°break point k=2ï¼ˆä»»æ„ä¸¤ä¸ªè¾“å…¥éƒ½ä¸èƒ½shatteredï¼‰:

è€ƒè™‘åˆ°æ­¤æ—¶$2^k=2^2=4$,å½“è¾“å…¥åŒ…å«4ç§dichotomiseæ—¶ï¼Œå°±è¦è€ƒè™‘æ˜¯å¦ä¼šæœ‰ä»»æ„ä¸¤ä¸ªè¾“å…¥ä¼šå‡ºç°shatterdçš„æƒ…å†µã€‚

$N\geq k$æ—¶ï¼Œå¢åŠ äº†break pointçš„é™åˆ¶åï¼Œç”Ÿäº§å‡½æ•°ä¼šå—åˆ°æå¤§å½±å“ï¼Œç»„åˆæ•°ä¼šæ€¥å‰§å‡å°‘ï¼Œç‰¹åˆ«çš„å½“k=1æ—¶ï¼Œ$m_H(N)=1$,ä¸ç®¡ä½ æœ‰å¤šå°‘ä¸ªè¾“å…¥ï¼Œåªè¦k=1ï¼Œåªè¦æœ‰ç¬¬äºŒä¸ªç»„åˆå‡ºç°ï¼Œå¿…å®šä¼šæœ‰æŸä¸ªè¾“å…¥è¿åçº¦æŸã€‚s

##### Bounding Functionï¼šBasic Cases

bounding function$B(N,k)$:

Nä¸ªè¾“å…¥åœ¨break pointæ˜¯kçš„æƒ…å†µä¸‹æœ€å¤šèƒ½æœ‰å¤šå°‘ç§ç»„åˆ

##### Bounding Functionï¼šInductive Cases

$B(N,k)\leq \sum_{i=0}^{k-1}(_i^N)$

æˆé•¿å‡½æ•°çš„ä¸Šé™ï¼Œä¸Šé™çš„ä¸Šé™

| B(N,k) | 1    | 2       | 3        | 4        | 5        | 6    |
| ------ | ---- | ------- | -------- | -------- | -------- | ---- |
| 1      | 1    | 2       | 2        | 2        | 2        | 2    |
| 2      | 1    | 3       | 4        | 4        | 4        | 4    |
| 3      | 1    | 4       | 7        | 8        | 8        | 8    |
| 4      | 1    | $\leq$5 | 11       | 15       | 16       | 16   |
| 5      | 1    | $\leq$6 | $\leq$16 | $\leq$26 | 31       | 32   |
| 6      | 1    | $\leq$7 | $\leq$22 | $\leq$42 | $\leq$57 | 63   |

ä¸Šè¡¨åˆ†4ç±»æƒ…å†µè¿›è¡Œè®¨è®ºï¼š

1. k=1

   $B(N,k)=1$

2. N<k

   $B(N,k)=2^N$

3. N=k

   $B(N,k)=2^N-1$ï¼Œåªè¦å»é™¤ä»»æ„çš„ä¸€ç§å³å¯æ»¡è¶³æ¡ä»¶

4. N>k

   $B(N,k)=2\alpha+\beta$

   $\alpha + \beta\leq B(N-1,k)$

   $\alpha\leq B(N-1,k-1)$

   â‡’$2\alpha + \beta\leq B(N-1,k)+B(N-1,k-1)$

ç»¼ä¸Š$B(N,k)\leq\sum_{i=0}^{k-1}(_i^N)$

å½“break pointå­˜åœ¨æ—¶ï¼Œæˆé•¿å‡½æ•°çš„å¤æ‚åº¦å°±æ˜¯å¤šé¡¹å¼çš„ã€‚

##### A Pictorial Proof

Vapnik-Chervonenkis (VC) bound:

$P[âˆ‹h\in H s.t.|E_{in}-E_{out}|>ğŸ„]\leq 4m_H(2N)e^{-\frac18NğŸ„^2}$

$E_{in}^\prime$Read as "$E_{in}$prime"



#### Lecture7:The VC Dimension

##### Definition of VC Dimension

æœ€å¤§çš„N $m_H(N)=2^N$

Hå¯ä»¥shatterçš„æœ€å¤§è¾“å…¥

$d_{vc}='minimum\space  k'-1$

æ¢è¨€ä¹‹ï¼Œå¦‚æœNä¸ªè¾“å…¥çš„ä»»æ„é›†åˆéƒ½ä¸èƒ½è¢«H shatteredï¼Œåˆ™$d_{vc}<N$,å¦‚æœå­˜åœ¨ä¸€ä¸ªNä¸ªè¾“å…¥çš„é›†åˆå¯ä»¥è¢«H shatteredï¼Œåˆ™$d_{vc}>N$

##### VC Dimention of Perceptrons

1D perceptron:$d_{vc}=2$

2D perceptrons:$d_{vc}=3$

d-D perceptrons:$d_{vc}=d+1$

> d+1æ˜¯perceptronçš„ç»´åº¦ï¼ˆåç½®+1ï¼‰

##### Physical Intuition of VC Dimension

è‡ªç”±åº¦ï¼Œå¯è‡ªç”±å˜åŒ–çš„é‡

$d_{vc}(H):powerfulness\space of\space  H$

##### Interpreting VC Dimension

æ ·æœ¬å¤æ‚åº¦ï¼ˆsample complexityï¼‰

$N=10d_{vc}$ç»å¸¸å°±èƒ½å­¦ä¹ åˆ°æ¯”è¾ƒå¥½çš„ç»“æœ



#### lecture8:Noise and Error

##### Noise and Probabilistic Target

å™ªå£°åŒ…æ‹¬è¯¯åˆ†ç±»ï¼Œé”™è¯¯æ ‡è®°ï¼Œä¸å®Œæ•´ä¿¡æ¯ç­‰ï¼Œé‚£ä¹ˆåœ¨æœ‰å™ªå£°çš„æ¡ä»¶ä¸‹ï¼ŒVC boundè¿˜æœ‰æ•ˆå—

åˆ†ç±»é”™è¯¯[[â€¦]]ä¹Ÿå«ä½œâ€œ0/1â€é”™è¯¯



##### Error Measure

###### Pointwise error measure

Pointwise:è¯„ä¼°æŸä¸€ä¸ªx
$$
E_{out}(g)=\epsilon_{x~p}[[g(x)\neq f(x)]],[[g(x)\neq f(x)]]è®°ä¸ºerr(g(x),f(x))
$$

###### ä¸¤ç§pointwise é”™è¯¯è¡¡é‡

0/1error:
$$
err(g(x),f(x))=[[g(x)\neq f(x)]]
$$
squared error:
$$
err(g(x),f(x))=(g(x)-f(x))^2
$$
é”™è¯¯è¡¡é‡ä¸åŒï¼Œæœ€å¥½çš„ç›®æ ‡ä¼šä¸åŒ

##### Algorithmic Error Measure

æ ¹æ®ç›®æ ‡çš„ä¸åŒï¼ˆè¶…å¸‚ä¿ƒé”€or CIAï¼‰ï¼Œç›¸åº”è°ƒæ•´é”™è¯¯è¡¡é‡



True:just err

Pausible:0/1çš„ä¼˜åŒ–æ˜¯ä¸€ä¸ªNPå›°éš¾é—®é¢˜ï¼Œå¹³æ–¹è¯¯å·®æ˜¯ä¸€ä¸ªæœ€å°åŒ–é«˜æ–¯å™ªå£°ï¼ˆGaussian noiseï¼‰çš„é—®é¢˜

Friendly:å®¹æ˜“ä¼˜åŒ–ï¼Œå°é—­å½¢å¼çš„è§£å†³æ–¹æ³•ï¼Œå‡¸å‡½æ•°

##### Weighted Classification

å¸¦æƒé‡çš„åˆ†ç±»é—®é¢˜å¯ä»¥è½¬åŒ–ä¸º0/1åˆ†ç±»é—®é¢˜ï¼Œæƒé‡å¯ä»¥è½¬åŒ–ä¸ºç›¸åŒæ•°ç›®çš„æ ·æœ¬ä¸ªæ•°ã€‚



#### Lecture9:Linear Regression

##### Linear Regression Problem

In-sample error:$E_{in}(w)=\frac1N\sum_{n=1}^N(h(x_n)-y_n)^2$

Out-of-sample error:$E_{out}(w)=\epsilon_{(x,y)~P}(w^Tx-y)^2$

å¦‚ä½•æœ€å°åŒ–$E_{in}(w)$?

##### Linear Regression Algorithm

æ¢¯åº¦ï¼Œå‡½æ•°åœ¨æ¯ä¸ªæ–¹å‘ä¸Šéƒ½ä½œåå¾®åˆ†

$min_w E_{in}(w)=\frac1N||Xw-y||^2$

æ‰¾åˆ°$w_{LIN}$ä½¿å¾—$\nabla E_{in}(w_{LIN})=0$

$E_{in}(w)=\frac1N||Xw-y||^2=\frac1N(w^TX^TXw-2w^TX^Ty+y^Ty)$

è®°$A=X^TX,b=X^Ty,c=y^Ty,å…¶ä¸­Aæ˜¯çŸ©é˜µï¼Œbæ˜¯å‘é‡ï¼Œcæ˜¯å¸¸æ•°$ï¼Œåˆ™ä¸Šå¼å¯ä»¥è¡¨è¾¾ä¸ºï¼š

$E_{in}(w)=\frac1N(w^TAw-2bw^T+c)$

$\nabla E_{in}(w)=\frac1N(2Aw-2b)=\frac2N(X^TXw-X^Ty)$

$w_{LIN}=(X^TX)^{-1}X^Ty$,è®°$(X^TX)^{-1}X^T$ ä¸º$X^å$(pseudo-inverse),åˆ™$w_{LIN}=X^åy$

##### Generalization Issue

##### Linear Regression for Binary Classification

çº¿æ€§åˆ†ç±»å’Œå›å½’çš„ä¸åŒåœ¨äºé”™è¯¯è¡¡é‡çš„ä¸åŒ



#### Lecture10:Logistic Regression

##### Logistic Regression Problem

äºŒåˆ†ç±»å„è‡ªçš„æ¦‚ç‡

##### Logistic Regression Error

cross-entropy errorï¼š$err(w,x,y)=\ln(1+\exp(-ywx))$

åˆ™E_in(w):
$$
E_{in}(w)=\frac1N\sum_{n=1}^N\ln(1+\exp(-y_nw^Tx_n))
$$
E_in(w)æ˜¯ä¸€ä¸ªè¿ç»­ï¼ˆcontinuousï¼‰ï¼Œå¯å¾®ï¼ˆdifferentiableï¼‰ï¼ŒäºŒæ¬¡å¯å¾®ï¼ˆtwice-differentiableï¼‰çš„å‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸º0çš„ç‚¹å³æœ€å°ç‚¹,ä¸‹é¢å¯¹wiæ±‚å‰ƒåº¦
$$
\cfrac{\partial E_{in}(w)}{\partial w_i}=\frac1N\sum_{n=1}^N(\cfrac{\partial \lnâ§ }{\partial â§ })(\cfrac{\partial (1+\exp(â¦¾))}{\partial â¦¾})(\cfrac{\partial(-y_nw^Tx_n)}{\partial w_i}) \\
=\frac1N\sum_{n=1}^N(\cfrac{1}{1+\exp(â¦¾)})(\exp(â¦¾))(-y_nx_{n,i})\\
=\frac1N\sum_{n=1}^N(\cfrac{\exp(â¦¾)}{1+\exp(â¦¾)})(-y_nx_{n,i})=\frac1N\sum_{n=1}^N\theta(â¦¾)(-y_nx_{n,i})\\
=\frac1N\sum_{n=1}^N\theta(-y_nw^Tx_n)(-y_nx_{n})
$$

##### Gradient of Logistic Regression Error

##### Gradient Descent

ä¸‹é™æœ€å¿«çš„åœ°æ–¹æ˜¯è´Ÿæ¢¯åº¦æ–¹å‘

###### $\eta$çš„é€‰æ‹©

æ­¥é•¿$\eta$å¤ªå°ï¼Œå¤ªæ…¢

æ­¥é•¿$\eta$å¤ªå¤§ï¼Œä¸ç¨³å®š

æœ€åé‡‡å–åŠ¨æ€è°ƒæ•´çš„ç­–ç•¥ï¼Œå…ˆå–è¾ƒå¤§çš„å€¼ï¼Œè¿­ä»£åˆ°ä¸€å®šæ¬¡æ•°åæ”¹ç”¨è¾ƒå°çš„é¢å€¼

æ­¥é•¿æœ€å¥½å’Œæ¢¯åº¦å€¼å‘ˆæ­£ç›¸å…³



è¿­ä»£ä»€ä¹ˆæ—¶å€™åœæ­¢ï¼Œç›´åˆ°å‰ƒåº¦ä¸º0ï¼Œæˆ–è€…æ˜¯è¿­ä»£è¶³å¤Ÿçš„æ¬¡æ•°åè®¤ä¸ºå·²ç»è¶³å¤Ÿå°äº†åˆ™åœæ­¢ã€‚



#### Lecture11:Linear Models for classification

##### Linear Models for Binary Classification

##### Stochastic Gradient Descent

éšæœºçš„

tè¶³å¤Ÿå¤§

0.1126ï¼Ÿ

##### Multiclass via Logistic Regression

one vs all

##### Multiclass via Binary Classifiction

one vs one



#### Lecture12:Nonlinear Transformation

çªç ´çº¿æ€§æ¨¡å‹çš„é™åˆ¶

##### Quadratic Hypotheses

å°†xç©ºé—´äºŒæ¬¡æ›²çº¿è½¬æ¢ä¸ºzç©ºé—´ç›´çº¿

##### Nolinear Transform

æ¢ä¸€ä¸ªç©ºé—´å»åšæ“…é•¿çš„çº¿æ€§åˆ’åˆ†

##### Price of Nonlinear Transform

##### Structured Hypothesis Sets



#### Lecture13:Hazard of Overfitting

##### What is Overfitting

è¿‡æ‹Ÿåˆå¯¼è‡´å¾ˆå·®çš„æ³›åŒ–èƒ½åŠ›

##### The Role of Noise and Data Size

â€‹                                                                                                                                                                                                                                                                                                                                    

##### Deterministic Noise



##### Dealing with Overfitting

ä»ç®€å•æ¨¡å‹å¼€å§‹

æ•°æ®æ¸…æ´—

æå–æ•°æ®æ›´å¤šçš„ä¿¡æ¯ï¼ˆæ•°æ®æç¤ºï¼‰

æ­£åˆ™åŒ–

éªŒè¯



#### Lecture14:Regularization

æ­£åˆ™åŒ–ï¼Œè§„åˆ™åŒ–

##### Regularized Hypothsis Set

çº¦æŸä¼˜åŒ–

sparseï¼ˆç¨€ç–çš„ï¼‰ hypothesis set ï¼ˆNP-hard to solveï¼‰

##### Weight Decay Regularization

Lagrange Multiplier

##### Regularization and VC Theory

augmented error

##### General Regularizers



#### Lecture15:Validation

##### Model Selection Problem

$$
goal: select\ H_{m^âˆ—} such\ that\ g_{m^âˆ—} = A_{m^âˆ—} (D)\ is\ of\ low\ E_{out(g_{m^âˆ— })}
$$

$$
m^âˆ— = argmin_{1\leq m\leq M}(E_m = E_{in}(A_m(D)))
$$

ç„¶è€Œä»…ä»…é€šè¿‡$E_{in}$  è¿›è¡Œé€‰æ‹©æ˜¯å¾ˆå±é™©çš„ï¼Œå› ä¸ºå®ƒçš„æ³›åŒ–èƒ½åŠ›å¯èƒ½å¾ˆå·®

å¦‚æœæ‰¾åˆ°ä¸€ä¸ªæ–°çš„æ•°æ®é›†D_test,é€šè¿‡E_testè¿›è¡Œé€‰æ‹©åˆä¼šæ€æ ·
$$
E_{out}(g_{mâˆ—} ) â‰¤ E_{test}(g_{mâˆ—} ) + O(\sqrt {\frac {log M}K})
$$
ä½†é—®é¢˜æ˜¯ä»å“ªé‡Œå»æ‰¾D_testå‘¢ï¼Œæˆ‘ä»¬ä¸å¯èƒ½å·²ç»çŸ¥é“è¦é¢„æµ‹çš„å†…å®¹ï¼Œç„¶åé’ˆå¯¹è¿™ä¸ªå†…å®¹å»å­¦ä¹ å‡ºä¸€ä¸ªé€‚åˆå®ƒçš„æ¨¡å‹ã€‚

é‚£ä¹ˆå…¶å®å¯ä»¥ä»åŸæ•°æ®é›†ä¸­ä¿ç•™ä¸€éƒ¨åˆ†D_val,è¿™éƒ¨åˆ†æ•°æ®ä¸å‚ä¸Aå¯¹Hé€‰æ‹©çš„è¿‡ç¨‹ã€‚

##### Validation

###### Validation Set D_val

DâD_train âˆª D_val(Kä¸ºD_valå¤§å°ï¼ŒNä¸ºDçš„å¤§å°)

$g_m=A_m(D)$		g_m^-=A_m(D_{train})

E_valå’ŒE_outæœ‰ä»€ä¹ˆè”ç³»å—ï¼Ÿ
$$
E_{out}(g_m^- ) â‰¤ E_{val}(g_{m}^- ) + O(\sqrt {\frac {log M}K})
$$
é€šè¿‡æœ€ä½³çš„E_valè¿›è¡Œæ¨¡å‹é€‰æ‹©
$$
E_{out}(g_{mâˆ—} ) â‰¤ E_{out}(g_m^- ) â‰¤ E_{val}(g_{m}^- ) + O(\sqrt {\frac {log M}K})
$$
Kçš„é€‰æ‹©å›°å¢ƒ

å¤§Kï¼šE_valâ‰ˆE_outï¼Œgm-çš„æ•ˆæœä¼šæ¯”gmå·®å¾ˆå¤š

å°K:gm-â‰ˆgm,ä½†æ˜¯E_valå’ŒE_outå·®åˆ«å¾ˆå¤§

ç»éªŒå€¼ï¼š

$K=\frac N5$



##### Leave-One-Out Cross Validation

##### V-Fold Cross Validation



#### Lecture16:Three Learning Principles

##### Occam's Razor

èƒ½å¤Ÿæœ€å¥½çš„é€‚åº”æ•°æ®çš„æœ€ç®€å•çš„æ¨¡å‹å°±æ˜¯æœ€åˆç†çš„ã€‚

ç®€å•hypothesis hï¼šçœ‹èµ·æ¥ç®€å•ï¼Œå•æ•°å¾ˆå°‘

ç®€å•æ¨¡å‹Hï¼šæ²¡æœ‰å¾ˆå¤šhypothwses

æ¨¡å‹å’Œå‡è®¾çš„å¤æ‚åº¦éƒ½å¾ˆä½



ç»å¸¸è¯¢é—®æ•°æ®æ˜¯ä¸æ˜¯è¢«è¿‡åˆ†çš„å»ºæ¨¡äº†ã€‚

##### Sampling Bias

##### Data Snooping

##### Power of Three

###### Three Related Fields

Data Mining

Artificial Intelligence

Statistics

###### Three Theoretical Bounds

Hoeffding

Multi-Bin Hoeffding

VC

###### Three Linear Models

PLA/pocket

Linear Regression

Logistic Regression

###### Three Key Tools

Feature Transform

Regularization

Validation

#### FAQ

çº¿æ€§ç›¸å…³ï¼Œå¯é€†,å‘é‡çš„å†…ç§¯æ˜¯å¯ä»¥äº¤æ¢çš„

5_7/27