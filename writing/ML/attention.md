# 深度学习中有益的概念hello

在面向不同的问题时，你所学习的知识的重要程度是有所差异的，比如曾经花费很多心血弄懂的卷积网络，在目标检测算法中仅仅是作为预训练特征提取而被一言带过，而有些你认为不重要的内容，却不管是在如何复杂的模型中，总是作为不可或缺的成分出现，比如深度学习中分别面向分类和回归的模型，不管表示部分的结构如何复杂，总而言之是为最终的分类回归问题服务的，而那几个经典的回归，则适时的出现，补上解决问题的最后一块拼图。

## Logistic Regression

### Binomial Logistic Regression 

一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是p，那么事件的几率是$\frac{p}{1-p}$,该事件的对数几率(log odds)或logit函数是$logit(p) = log\frac{p}{1-p}$.对Logistic Regression而言：
$$
log\frac{p(Y=1|X)}{1-p(Y=1|X)}=\vec{\omega}\cdot \vec{x}
$$
输出Y=1的对数几率是输入x的线性模型，使用极大似然估计法估计参数模型，求的似然函数最大时，$\vec\omega$的值.

## CNN

**输入及中间特征图**

卷积层堆叠特征图与定义卷积层的张量有关，一个常规卷积网络的输入（可以由一个3-D tensor定义，[height,width, channels],则一个mini-batch 则可由一个4-D张量定义，4-D tensor of shape [mini-batchsize, height, width, channels]）。



**卷积层参数**

卷积层的参数也是用一个4D的张量表示

4-D tensor of shape [fh, fw, fn, fn′]，偏移项是 1-D tensor of shape [fn]张量的四个维度分别代表：前2核大小，前一层特征图数或输入通道数（同时逐一对应一个偏移项），最末是输出特征图数目

 

**具体每个特征图的大小则由多个因素共同决定**.



## 评价

|      | T    | F    |
| ---- | ---- | ---- |
| T    | TP   | FN   |
| F    | FP   | TN   |

精确率定义为：P=TP/（TP+FP）

召回率定义为：R=TP/（TP+FN）

## 误差和方差

Variance则是“不同的训练数据集训练出的模型”的输出值之间的差异。

稳定的偏离很远（高误差低方差）

### 典型模型中消除过拟合及欠拟合的方式

1. 欠拟合——增加模型复杂度
   - 1）增加模型迭代次数；
   - 2）训练一个复杂度更高的模型：比如在神经网络中增加神经网络层数、在SVM中用非线性SVM（核技术）代替线性SVM
   - 3）获取更多的特征以供训练使用：特征少，对模型信息的刻画就不足够了
   - 4）降低正则化权重：正则化正是为了限制模型的灵活度（复杂度）而设定的，降低其权值可以在模型训练中增加模型复杂度。
2. 过拟合——降低模型复杂度
   - 1）获取更多的数据：训练数据集和验证数据集是随机选取的，它们有不同的特征，以致在验证数据集上误差很高。更多的数据可以减小这种随机性的影响。
   - 2）减少特征数量
   - 3）增加正则化权重：方差很高时，模型对训练集的拟合很好。实际上，模型很有可能拟合了训练数据集的噪声，拿到验证集上拟合效果就不好了。我们可以增加正则化权重，减小模型的复杂度。