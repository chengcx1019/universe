机器学习最适用的场景包括模式识别，异常识别及预测

**传统机器学习模型分类**

抛开神经网络NN(我们后面再谈)，机器学习可分为四大类，分别是监督学习，非监督学习，半监督学习及迁移学习

1. 监督学习{

   1. 分类算法{

      - 感知机

      - KNN

      - 概率模型{

        - 朴素贝叶斯(NB)
        - Logistic Regression(LR)
        - 最大熵MEM(与LR同属对数线性分类模型)

        }

      - 支持向量机

      - 决策树(ID3,CART,C4.5)

      - 集成学习(assembly learning){

        - Boosting{

          - Gradient Boosting{

            - GBDT
            - XGBoost

            }

          - AdaBoost

          }

        - Bagging:{

          - 随机森林

          }

        - Stacking

        }

        ...

      }

   2. 概率图模型{

      - HMM

      - MEMM(最大熵马尔可夫)

      - CRF

        ...

      }

   3. 回归预测{

      - 线性回归
      - 树回归
      - Ridge岭回归
      - Lasso回归

      ...

      }

   ...

   }

2. 非监督学习{

   1. 聚类{

      - 基础聚类
        - K-means
        - 二分K-means
        - K中值聚类
        - GMM聚类
      - 层次聚类
      - 密度聚类
      - 谱聚类

      }

   2. 主题模型

      - pLSA
      - LDA隐含狄利克雷分析

   3. 关联分析

      - Apriori算法
      - FP-growth算法

   4. 降维

      - PCA
      - SVD
      - LDA线性判别分析
      - LLE局部线性嵌入

   5. 异常检测

      ...

   }

3. 半监督学习

4. 迁移学习

 **关于上面的一些注解**：

1. 概率图模型可以进一步划分

   - 有向模型：也称信念网络或者贝叶斯网络
   - 无向模型：马尔可夫随机场或马尔可夫网络

2. 几种概率图模型的比较

   - HMM(隐马尔可夫模型)

     HMM模型有两个限制，一是观测值之间是独立的，二是状态转移过程中当前状态仅与前一状态有关；现实情况中很多问题是不满足这两个限制的，很多情况都需要基于范围更广的上下文进行判定。

   - MEMM(最大熵马尔可夫模型)

     MEMM模型解决了观测独立的问题，MEMM引入自定义特征函数，不仅可以表达观测间的依赖，还可以表示当前观测和前后多个状态之间的复杂依赖关系。

     但是MEMM也存在自身问题，因为只在局部做归一化，MEMM容易陷入局部最优

   - CRF

     CRF统计全局概率，不存在MEMM的标记偏置的问题

     同时CRF属于无向图，不存在有向图的依赖关系

3. 补充说明的是几种集成学习的差异（TODO 20190618）



**统计机器学习的学习流程**

a.训练模型参数，得到模型（由参数唯一确定），
b.预测给定的测试数据。

**解决决策问题的三种方法**

1. 生成模型

   针对每个类别，独立的确定类条件密度，然后根据类先验概率，利用贝叶斯定理求出后验概率，当然也可以直接对联合概率进行建模，然后归一化得到后验概率

   **联合概率分布-类别先验概率（从观测数据获得Y的概率分布）和类别条件概率**

2. 判别模型：直接对后验概率进行建模的方式称为判别式模型

   对于判别模型，是由$p(y|x)$的条件分布决定的而这个条件分布本身是由模型参数$\theta$决定的，因而整个问题就可以转化为在给定观测数据的情况下，求$\theta$的后验概率分布，给定$\hat{\theta}$和观测数据，得到条件分布关于观测数据的极大似然估计

   条件分布，似然函数最大化

   **条件分布-最大后验概率估计-似然函数-极大似然**

3. 决策函数：找到一个函数直接把输入映射到标签

据此，可以将之前分类的监督模型进行划分(TODO 20190618)

- 分类
  - A批模型（神经网络模型、SVM、perceptron、LR、DT……）判别模型
  - B批模型（NB、LDA……）

**神经网络架构分为三类**

- 前馈神经网络

  这是实际应用中最常见的神经网络类型。第一层是输入，最后一层是输出。如果有多个隐藏层，我们称之为「深度」神经网络。各层神经元的活动是前一层活动的非线性函数。

- 循环网络

  循环网络在他们的连接图中定向了循环，这意味着你可以按照箭头回到你开始的地方。他们可以有复杂的动态，使其很难训练。他们更具有生物真实性。

  目前如何高效地训练循环网络正在受到广泛关注。循环神经网络是模拟连续数据的一种非常自然的方式。它们相当于每个时间片段具有一个隐藏层的深度网络；除此之外，它们在每个时间片段上使用相同的权重并且在每个时间片段上输入。它们可以长时间记住隐藏状态的信息，但很难训练其使用这个潜能。

- 对称连接网络

  对称连接网络有点像循环网络，但是单元之间的连接是对称的（它们在两个方向上权重相同）。比起循环网络，对称连接网络更容易分析。这个网络中有更多的限制，因为它们遵守能量函数定律。没有隐藏单元的对称连接网络被称为「Hopfield 网络」。有隐藏单元的对称连接的网络被称为玻尔兹曼机。

**8种神经网络架构**

1. 感知器

   单个神经元的计算模型

2. 卷积神经网络

3. 循环神经网络

4. 长短期记忆网络

5. Hopfield 网络

6. 玻尔兹曼机网络

7. 深度信念网络

   梯度和反向传播的差异

   信念网是由随机变量组成的有向无环图。使用信念网我们可以观察到一些变量。我们想要解决 2 个问题：1）推理问题：推断未观测变量的状态; 2）学习问题：调整变量之间的交互作用，使网络更有可能产生训练数据。 

8. 深度自动编码器

   比反向传播更好的训练方式：无监督的逐层预训练



**用于推荐系统的所有深度方法**(TODO 20190618)