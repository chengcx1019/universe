## Hsuan-Tien Lin——Machine Learning Foundations

When can machines learn？Lecture1-4

Why can machines learn？Lecture5-8

How can machines learn？Lecture9-12

How can machines learn better？Lecture13-16

#### Lecture1:The Learning Problem

A takes D and H to get g.

##### Course Introduction

##### What is Machine Learning

machine learning:improving some performance measure with experience from data.

通过从数据计算出的经验来提升表现



key essence（本质）

1. 存在可以学习的隐藏模式
2. 没有可易编程的定义
3. 有可供学习模式的数据

##### Applications of Machine Learning

应用：计算经济，图片识别

##### Components of Machine Learning

use data  to compute hypothesis g that approximates target f

##### Machine Learning and other Fields

ML——Data Mining

传统的数据挖掘更关心大数据量的高效计算

ML——Artificial Intelligence

ML是实现AI的一种途径

ML——Statistics

传统的统计分析关注点更多是在结果的数学假设证明



#### Lecture2:Learning to Answer Yse/No

##### Perceptron Hypothesis Set

$h(x)=sign((\sum_{i=1}^dw_ix_i)-threshold)$

$=sign((\sum_{i=1}^dw_ix_i)+(-threshold)_{w0}*(+1)_{x0})$

$=sign(\sum_{i=0}^dw_ix_i))=sign(w^Tx)$

##### Perceptron Learning Algorithm(PLA)

在数据集线性可分的基础上，寻找误分类，修正w直到没有误分类点

$sign(w^Tx_n)\neq y_n,w_{t+1}^T<—w_t+y_nx_n$

perceptron——linear

误分类点距离最小

A fault confessed is half redressed.知错能改，善莫大焉

issues of PLA（perceptron learning algorithm）：

1. 算法何时停止（linear separable线性可分）
2. 准确率到底有高

$y_nw_{t+1}^Tx_n\geq y_nw_t^Tx_n$

##### Guarantee of PLA

设经过T次迭代$w_f$将训练数据集完成正确分开

记$R^2=max_{1\leq i\leq n}||x_i||^2,\rho=min_iy_i\frac{W_f^T}{||w_f||}x_i$

$sign(w_t^Tx_{n(t)})\neq y_{n(t)},y_{n(t)}w^Tx_{n(t)}\leq 0$



$||w_{t+1}^T||^2=||w_t+y_{n(t)}x_{n(t)}||^2$

$=||w_t||^2+2y_{n(t)}w^Tx_{n(t)}+||y_{n(t)}x_{n(t)}||^2$

$\leq ||w_t||^2+0+||y_{n(t)}x_{n(t)}||^2$

$\leq ||w_t||^2+0+R^2$

$\leq ||w_{t-1}||^2+2R^2 \leq...$

$\leq (T+1)R^2$



$w_f^Tw_{t+1}=w_f^T(w_t+y_{n(t)}x_{n(t)})$

$\geq w_f^Tw_t +||w_f||\rho$

$\geq (T+1)||w_f||\rho$



$1\geq \frac{w_f^T}{||w_f||}\frac {w_T}{||w_T||}\geq \frac{T||w_f||\rho}{||w_f||\sqrt[2]TR}=\sqrt[2]T\frac{\rho}R$

$T\leq\frac{R^2}{\rho^2}$

##### Non-separable Data

如果数据集D不是线性可分，有噪声数据（noisy data）时该如何学习

$w_g<—argmin_w\sum_{n=1}^N[[y_n\neq sign(w^Tx_n)]]$(犯错误最小的)

这是NP-hard问题

退而求其次，使用pocket algorithm，每次保留最好的，迭代到一定次数就认为达到最优了。



#### Lecture3:Types of Learning

##### Learning wth Different Output Space Y

多分类

回归

##### Learning wth Different Data Label $y_n$

###### Unsipervised Learning Problems

unsupervised multiclass classification——clustering

没有标记，聚类

Density estimation

Outline detection

###### Semi-supervised

只有一部分标记，标记的代价太大

###### Reinforcement Learning

##### Learning wth Different Protocol $f ⇒(x_n,y_n)$

batch:填鸭式，所有数据

online：hypothesis improves through receiving data instances   sequentially.

active

random

##### Learning wth Different Input Space X

concrete features：sophisticated（and related） physical meaning

raw features：simple physical meaning

abstract feature：no pyhsical meaning，



#### Lecture4:Feasibility of Learning

##### Learning is Impossible

##### Probability to the Rescue

𝞶是抽样某一类的概率，𝞵是原始数据中该类的概率，一般而言，𝞶与𝞵有什么关系，可否用𝞶来描述𝞵？

当N的值足够大，则可以说v与u是接近的

$P[|𝞶-𝞵|>𝟄]\leq 2e^{-2N𝟄^2}$

上式称为Hoeffding’s Inequality（霍夫丁不等式）

表达式$'𝞶=𝞵'$的意义是差不多大概是正确的（probably approximately correct,PAC）

##### Connection to Learning

前提：N足够大并且是i.i.d 独立取样

Unknown $E_{out}(h)=\epsilon_{x~p}[[]h(x)\neq f(x)]$ 

by known $E_{in}(h)=\frac1N\sum_{n=1}^N[[h(x_n)\neq y_n]]$

$E_{in}$样本错误率，$E_{out}$样本之外数据的错误率

$P[|E_{in}-E_{out}|>𝟄]\leq 2e^{-2N𝟄^2}$

##### Connection to Real Learning

###### bad sample and bad data

$P_D[BAD D]= \sum_{all possible D}P(D)∙[[BAD D]]$



$x_i相互独立,且x_i\in [a_i,b_i],S_n=x_1+x_2+···+x_n$$ 则有:$$P(S_n-E[S_n]\geq t)\leq exp(-\frac {2t^2}{\sum_{1}^n{(b_i-a_i)^2}})$

###### Bound of Bad Data

$$P_D[BAD D]= P_D[BADD for h_1 or BADD for h_2…orBADD for h_M]$$

$$\leq P_D[BADD for h_1]+P_D[BADD for h_2]+…+P_D[BADD for h_M]$$

$$\leq2e^{-2N\epsilon^2}+2e^{-2N\epsilon^2}+…+2e^{-2N\epsilon^2} $$

$$=2Me^{-2N\epsilon^2}$$



#### Lecture5:Training versus Testing

到底为什么机器可以学到东西

##### Recap and preview

学习的两个核心问题

1. $E_{out}(g)$是否足够接近$E_{in}(g)$
2. 能否使$E_{in}(g)$足够小

$M$即$|H|$在以上两个问题中起着什么作用

M小得时候：接近但选择有限

M大的时候：很难保证二者接近，但有更多选择



$P[|E_{in}-E_{out}|>𝟄]\leq 2Me^{-2N𝟄^2}$

记$2Me^{-2N𝟄^2}=\delta$,如果使$\delta=0.05$,即5%的错误容忍情况下，$\epsilon=0.1.M=100$的情况下，需要的数据大小应该多少？$N=\frac1{2\epsilon^2}\ln{\frac{2M}\delta}$

##### Effective Number of Lines

在hypothesis set $H={all\space  lines\space in\space R^2}$中,可以根据数据集对线进行分类，找到那些真正有效的线。

找出相似假设，分类情况是一致的，如1个点，2种分类，2个点，最多4种分类情况，3个点，最多8种分类情形（点共线	），4个点（仅列举8种情况1111 1110  1101 1100 1000 1001 1010 1011）

N个输入最大分类种数即有效的线的数目，记为：$effective(N)$

$P[|E_{in}-E_{out}|>𝟄]\leq 2effective(N)e^{-2N𝟄^2}$

##### Effective Number of Hypothese

###### Dichotomies:Mini-hypotheses

dichotomy二分

|      | hypotheses H       | dichotomise $H(x_1,x_2,…,x_n)$ |
| ---- | ------------------ | ------------------------------ |
| e.g. | all lines in $R^2$ | 点的取值不同组合                       |
| Size | possibly infinite  | Upper bounded by $2^N$         |

###### Growth Function

成长函数(growth function)：取不同N时组合的最大值

$$m_H(N)=max_{x_1,x_2,…,x_N\in X |H(x_1,x_2,…,x_n)|}$$

如何计算成长函数，以下是几种典型模型的成长函数：

**positive rays**：$ h(x)=sign(x−a) $输入空间为一维向量， 当$x_i≥a$, 输出 +1, 当$x_i<a$ 时， 输出 -1. 

$m_H(N)=N+1$(N个点将区域划分为（N+1）个部分)，但如果有两个点同时发生的话，就没有这么多情况了，但这对成长函数是没有影响的，因为成长函数本身是取分类最多的情况。

$m_H(2)=3\leq 2^2:break\space point\space at\space2$

**positive intervals**：$当 x_i∈[l,r),y_i=1;x_i\notin [l,r),y_i=−1$

$m_H(N)=(_2^{N+1})=\frac12N^2+\frac12N+1$

N个点将区域划分为 N+1个空间， N+1个空间任取两个空间进行划分，但如果两者重合的话，对应于一种划分(均为-1)。

$m_H(3)=7\leq 2^3:break\space point\space at\space3$

**convex set(凸集)**：h(x)=+1iffxina convex region,  1 otherwise

$m_H(N)=2^N$

$m_H(N)=2^N\space always:no\space break\space point\space$

**2D perceptrons**:

$m_H(N)<2^N$

$m_H(4)=14\leq 2^4:break\space point\space at\space4$

**Positive and negative rays(1D perceptrons)**:等价于1维的perceptron hypothsis

$n_H=2(N-1)+2$

N个点中间的（N-1）个区域可以分别正向和反向，全正全反各一（但是都对应于边界区域的两种情况正正-1，负负-1，正负+1，负正+1）

$m_H(3)=6\leq 2^3:break\space point\space at\space3$

##### Break Point

多项式（polynomial）还是指数型（exponential）

如果 k个输入没有被H完全划分，则称k为H的break point



#### Lecture6: Theory of Generalization

##### Restriction of Break Point

> 前面5种模型的braek point均在L5_groth function中给出

成长函数中增加break point的限制

<u>maximum possible mH(N) when N = 3 and k = 2?</u>

如果最小break point k=2（任意两个输入都不能shattered）:

考虑到此时$2^k=2^2=4$,当输入包含4种dichotomise时，就要考虑是否会有任意两个输入会出现shatterd的情况。

$N\geq k$时，增加了break point的限制后，生产函数会受到极大影响，组合数会急剧减少，特别的当k=1时，$m_H(N)=1$,不管你有多少个输入，只要k=1，只要有第二个组合出现，必定会有某个输入违反约束。s

##### Bounding Function：Basic Cases

bounding function$B(N,k)$:

N个输入在break point是k的情况下最多能有多少种组合

##### Bounding Function：Inductive Cases

$B(N,k)\leq \sum_{i=0}^{k-1}(_i^N)$

成长函数的上限，上限的上限

| B(N,k) | 1    | 2       | 3        | 4        | 5        | 6    |
| ------ | ---- | ------- | -------- | -------- | -------- | ---- |
| 1      | 1    | 2       | 2        | 2        | 2        | 2    |
| 2      | 1    | 3       | 4        | 4        | 4        | 4    |
| 3      | 1    | 4       | 7        | 8        | 8        | 8    |
| 4      | 1    | $\leq$5 | 11       | 15       | 16       | 16   |
| 5      | 1    | $\leq$6 | $\leq$16 | $\leq$26 | 31       | 32   |
| 6      | 1    | $\leq$7 | $\leq$22 | $\leq$42 | $\leq$57 | 63   |

上表分4类情况进行讨论：

1. k=1

   $B(N,k)=1$

2. N<k

   $B(N,k)=2^N$

3. N=k

   $B(N,k)=2^N-1$，只要去除任意的一种即可满足条件

4. N>k

   $B(N,k)=2\alpha+\beta$

   $\alpha + \beta\leq B(N-1,k)$

   $\alpha\leq B(N-1,k-1)$

   ⇒$2\alpha + \beta\leq B(N-1,k)+B(N-1,k-1)$

综上$B(N,k)\leq\sum_{i=0}^{k-1}(_i^N)$

当break point存在时，成长函数的复杂度就是多项式的。

##### A Pictorial Proof

Vapnik-Chervonenkis (VC) bound:

$P[∋h\in H s.t.|E_{in}-E_{out}|>𝟄]\leq 4m_H(2N)e^{-\frac18N𝟄^2}$

$E_{in}^\prime$Read as "$E_{in}$prime"



#### Lecture7:The VC Dimension

##### Definition of VC Dimension

最大的N $m_H(N)=2^N$

H可以shatter的最大输入

$d_{vc}='minimum\space  k'-1$

换言之，如果N个输入的任意集合都不能被H shattered，则$d_{vc}<N$,如果存在一个N个输入的集合可以被H shattered，则$d_{vc}>N$

##### VC Dimention of Perceptrons

1D perceptron:$d_{vc}=2$

2D perceptrons:$d_{vc}=3$

d-D perceptrons:$d_{vc}=d+1$

> d+1是perceptron的维度（偏置+1）

##### Physical Intuition of VC Dimension

自由度，可自由变化的量

$d_{vc}(H):powerfulness\space of\space  H$

##### Interpreting VC Dimension

样本复杂度（sample complexity）

$N=10d_{vc}$经常就能学习到比较好的结果



#### lecture8:Noise and Error

##### Noise and Probabilistic Target

噪声包括误分类，错误标记，不完整信息等，那么在有噪声的条件下，VC bound还有效吗

分类错误[[…]]也叫作“0/1”错误



##### Error Measure

###### Pointwise error measure

Pointwise:评估某一个x
$$
E_{out}(g)=\epsilon_{x~p}[[g(x)\neq f(x)]],[[g(x)\neq f(x)]]记为err(g(x),f(x))
$$

###### 两种pointwise 错误衡量

0/1error:
$$
err(g(x),f(x))=[[g(x)\neq f(x)]]
$$
squared error:
$$
err(g(x),f(x))=(g(x)-f(x))^2
$$
错误衡量不同，最好的目标会不同

##### Algorithmic Error Measure

根据目标的不同（超市促销or CIA），相应调整错误衡量



True:just err

Pausible:0/1的优化是一个NP困难问题，平方误差是一个最小化高斯噪声（Gaussian noise）的问题

Friendly:容易优化，封闭形式的解决方法，凸函数

##### Weighted Classification

带权重的分类问题可以转化为0/1分类问题，权重可以转化为相同数目的样本个数。



#### Lecture9:Linear Regression

##### Linear Regression Problem

In-sample error:$E_{in}(w)=\frac1N\sum_{n=1}^N(h(x_n)-y_n)^2$

Out-of-sample error:$E_{out}(w)=\epsilon_{(x,y)~P}(w^Tx-y)^2$

如何最小化$E_{in}(w)$?

##### Linear Regression Algorithm

梯度，函数在每个方向上都作偏微分

$min_w E_{in}(w)=\frac1N||Xw-y||^2$

找到$w_{LIN}$使得$\nabla E_{in}(w_{LIN})=0$

$E_{in}(w)=\frac1N||Xw-y||^2=\frac1N(w^TX^TXw-2w^TX^Ty+y^Ty)$

记$A=X^TX,b=X^Ty,c=y^Ty,其中A是矩阵，b是向量，c是常数$，则上式可以表达为：

$E_{in}(w)=\frac1N(w^TAw-2bw^T+c)$

$\nabla E_{in}(w)=\frac1N(2Aw-2b)=\frac2N(X^TXw-X^Ty)$

$w_{LIN}=(X^TX)^{-1}X^Ty$,记$(X^TX)^{-1}X^T$ 为$X^十$(pseudo-inverse),则$w_{LIN}=X^十y$

##### Generalization Issue

##### Linear Regression for Binary Classification

线性分类和回归的不同在于错误衡量的不同



#### Lecture10:Logistic Regression

##### Logistic Regression Problem

二分类各自的概率

##### Logistic Regression Error

cross-entropy error：$err(w,x,y)=\ln(1+\exp(-ywx))$

则E_in(w):
$$
E_{in}(w)=\frac1N\sum_{n=1}^N\ln(1+\exp(-y_nw^Tx_n))
$$
E_in(w)是一个连续（continuous），可微（differentiable），二次可微（twice-differentiable）的凸函数，梯度为0的点即最小点,下面对wi求剃度
$$
\cfrac{\partial E_{in}(w)}{\partial w_i}=\frac1N\sum_{n=1}^N(\cfrac{\partial \ln⧠}{\partial ⧠})(\cfrac{\partial (1+\exp(⦾))}{\partial ⦾})(\cfrac{\partial(-y_nw^Tx_n)}{\partial w_i}) \\
=\frac1N\sum_{n=1}^N(\cfrac{1}{1+\exp(⦾)})(\exp(⦾))(-y_nx_{n,i})\\
=\frac1N\sum_{n=1}^N(\cfrac{\exp(⦾)}{1+\exp(⦾)})(-y_nx_{n,i})=\frac1N\sum_{n=1}^N\theta(⦾)(-y_nx_{n,i})\\
=\frac1N\sum_{n=1}^N\theta(-y_nw^Tx_n)(-y_nx_{n})
$$

##### Gradient of Logistic Regression Error

##### Gradient Descent

下降最快的地方是负梯度方向

###### $\eta$的选择

步长$\eta$太小，太慢

步长$\eta$太大，不稳定

最后采取动态调整的策略，先取较大的值，迭代到一定次数后改用较小的额值

步长最好和梯度值呈正相关



迭代什么时候停止，直到剃度为0，或者是迭代足够的次数后认为已经足够小了则停止。



#### Lecture11:Linear Models for classification

##### Linear Models for Binary Classification

##### Stochastic Gradient Descent

随机的

t足够大

0.1126？

##### Multiclass via Logistic Regression

one vs all

##### Multiclass via Binary Classifiction

one vs one



#### Lecture12:Nonlinear Transformation

突破线性模型的限制

##### Quadratic Hypotheses

将x空间二次曲线转换为z空间直线

##### Nolinear Transform

换一个空间去做擅长的线性划分

##### Price of Nonlinear Transform

##### Structured Hypothesis Sets



#### Lecture13:Hazard of Overfitting

##### What is Overfitting

过拟合导致很差的泛化能力

##### The Role of Noise and Data Size

​                                                                                                                                                                                                                                                                                                                                    

##### Deterministic Noise



##### Dealing with Overfitting

从简单模型开始

数据清洗

提取数据更多的信息（数据提示）

正则化

验证



#### Lecture14:Regularization

正则化，规则化

##### Regularized Hypothsis Set

约束优化

sparse（稀疏的） hypothesis set （NP-hard to solve）

##### Weight Decay Regularization

Lagrange Multiplier

##### Regularization and VC Theory

augmented error

##### General Regularizers



#### Lecture15:Validation

##### Model Selection Problem

$$
goal: select\ H_{m^∗} such\ that\ g_{m^∗} = A_{m^∗} (D)\ is\ of\ low\ E_{out(g_{m^∗ })}
$$

$$
m^∗ = argmin_{1\leq m\leq M}(E_m = E_{in}(A_m(D)))
$$

然而仅仅通过$E_{in}$  进行选择是很危险的，因为它的泛化能力可能很差

如果找到一个新的数据集D_test,通过E_test进行选择又会怎样
$$
E_{out}(g_{m∗} ) ≤ E_{test}(g_{m∗} ) + O(\sqrt {\frac {log M}K})
$$
但问题是从哪里去找D_test呢，我们不可能已经知道要预测的内容，然后针对这个内容去学习出一个适合它的模型。

那么其实可以从原数据集中保留一部分D_val,这部分数据不参与A对H选择的过程。

##### Validation

###### Validation Set D_val

D➝D_train ∪ D_val(K为D_val大小，N为D的大小)

$g_m=A_m(D)$		g_m^-=A_m(D_{train})

E_val和E_out有什么联系吗？
$$
E_{out}(g_m^- ) ≤ E_{val}(g_{m}^- ) + O(\sqrt {\frac {log M}K})
$$
通过最佳的E_val进行模型选择
$$
E_{out}(g_{m∗} ) ≤ E_{out}(g_m^- ) ≤ E_{val}(g_{m}^- ) + O(\sqrt {\frac {log M}K})
$$
K的选择困境

大K：E_val≈E_out，gm-的效果会比gm差很多

小K:gm-≈gm,但是E_val和E_out差别很大

经验值：

$K=\frac N5$



##### Leave-One-Out Cross Validation

##### V-Fold Cross Validation



#### Lecture16:Three Learning Principles

##### Occam's Razor

能够最好的适应数据的最简单的模型就是最合理的。

简单hypothesis h：看起来简单，单数很少

简单模型H：没有很多hypothwses

模型和假设的复杂度都很低



经常询问数据是不是被过分的建模了。

##### Sampling Bias

##### Data Snooping

##### Power of Three

###### Three Related Fields

Data Mining

Artificial Intelligence

Statistics

###### Three Theoretical Bounds

Hoeffding

Multi-Bin Hoeffding

VC

###### Three Linear Models

PLA/pocket

Linear Regression

Logistic Regression

###### Three Key Tools

Feature Transform

Regularization

Validation

#### FAQ

线性相关，可逆,向量的内积是可以交换的

5_7/27